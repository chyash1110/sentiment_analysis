# -*- coding: utf-8 -*-
"""twitter_sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/126t3BcQt1F3bWvNKHQN0lhOJraFMF2dx

Import necessary libraries
"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import twitter_samples
import nltk
nltk.download('twitter_samples')

import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import nltk
nltk.download('stopwords')

"""Load DataSet"""

all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

"""Split Dataset into 80% training and 20% test set"""

train_pos = all_positive_tweets[:4000]
test_pos = all_positive_tweets[4000:]

train_neg = all_negative_tweets[:4000]
test_neg = all_negative_tweets[4000:]

train_x = train_pos + train_neg
test_x = test_pos + test_neg

train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)
test_y = np.append(np.ones((len(test_pos),1)), np.zeros((len(test_neg),1)), axis=0)

"""# Preprocess Tweets
1. Remove URLs, twitter marks and styles
2. Tokenize and Lowercase
3. Remove stopwords and punctuation
4. Stemming
"""
def process_tweet(tweet):
    stopwords_english = stopwords.words('english')
    stemmer = PorterStemmer()
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    tweet = re.sub(r'#', '', tweet)
    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)
    tweet_tokens = tokenizer.tokenize(tweet)
    tweet_stem = []

    for word in tweet_tokens:
        if word not in stopwords_english and word not in string.punctuation:
            tweet_stem.append(word)
    return tweet_stem


"""# Build Frequencies
1. Build vocabulory dictionary from training data in the form of {(word,label):freq}.
2. train_x & train_y is the corpus of tweets that is used to build frequency dictionary
3. Vocabulory is the set of unique words from corpus and its positive frequency is the number of times that word has appeared in positive tweets and negative frequency is the number of times that word has appeared in negative tweets.
"""

def build_freqs(tweets, labels):
    labels_list = np.squeeze(labels).tolist()
    freqs = {}
    for label,tweet in zip(labels_list,tweets):
        for word in process_tweet(tweet):
            pair = (word,label)
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1
    return freqs
freqs = build_freqs(train_x, train_y)

"""# Extract Features
1. Features are extracted from frequency dictionary.
2. Feature of tweet m: X_m = [1, sum of positive frequencies, sum of negative frequencies]
"""

def extract_features(tweet, freqs):
    words = process_tweet(tweet)
    x = np.zeros((1, 3))
    x[0, 0] = 1
    for word in words:
        if (word, 1.0) in freqs.keys():
            x[0, 1] += freqs[(word, 1.0)]
        if (word, 0.0) in freqs.keys():
            x[0, 2] += freqs[(word, 0.0)]
    return x

X = np.zeros((len(train_x), 3))
for i in range(len(train_x)):
    X[i,:] = extract_features(train_x[i], freqs)

"""# Train Logistic Regression Model
1. Write gradient descent function to minimize cost of training
2. Call Gradient Descent function on training features X
"""

def gradient_descent(x, y, theta, alpha, num_iteration):
    m = len(x)
    for i in range(0,num_iteration):
        z = np.dot(x,theta)
        h = 1/(1 + np.exp(-z))
        J = (-1/float(m))*(np.dot(np.transpose(y), np.log(h)) + np.dot(np.transpose(1-y), np.log(1-h)))
        theta = theta - (alpha/m)*np.dot(np.transpose(x), (h-y))
    J = float(J)
    return J,theta
J,theta = gradient_descent(X, train_y, np.zeros((3,1)), 1e-9, 1500)

"""# Test Logistic Regression Model
1. write predict_tweet() function to predict the sentiment of tweet
2. write test_logistic_regression function to evaluate the model on test data
"""

def predict_tweet(tweet, freqs, theta):
    x = extract_features(tweet, freqs)
    z = np.dot(x, theta)
    y_predict = 1 / (1 + np.exp(-z))
    return y_predict[0, 0]

def test_logistic_regression(test_x, test_y, freqs, theta):
    y_hat = []
    for tweet in test_x:
        y_predict = predict_tweet(tweet, freqs, theta)
        if y_predict >= 0.5: 
            y_hat.append(1.0)
        else:
            y_hat.append(0)
    test_y = np.squeeze(test_y)
    accuracy = sum(y_hat == test_y) / len(test_y)
    return accuracy * 100

test_logistic_regression(test_x, test_y, freqs, theta)